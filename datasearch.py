# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-UdYpDkydNyx2DwvlmlljOLbbSf0eabb
"""

# !pip install gensim
# !pip install sklearn
# !pip install nltk
# !pip install hashedindex
import nltk
import string
import hashedindex
import pandas as pd
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('words')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from hashedindex import HashedIndex
from string import printable
import string
import gensim
import math
from gensim import corpora
from gensim.similarities import SparseMatrixSimilarity
from gensim.models import TfidfModel
from gensim.models import LsiModel
from gensim.summarization.bm25 import BM25
from string import printable

## Import wikiretrieve module
from wikiretrieve import WikiRetrieve
wr = WikiRetrieve()
url_input = input('Enter a valid URL: ')
dataframe = wr.data2df(url_input)
printableSet = set(printable)
dataframe['NONASCIIContent'] = dataframe['Content'].apply(lambda x: ''.join([' ' if item not in printableSet else item for item in x]))
# dataframe['NONASCIIContent']
stop_words = stopwords.words('english')
dataframe['RM_StopwordsContent'] = dataframe['NONASCIIContent'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])
# dataframe['RM_StopwordsContent']
def remove_punct(x):
    punct = str(x).translate(str.maketrans('', '', string.punctuation))
    return punct
dataframe['Rm_Punctuation'] = dataframe['RM_StopwordsContent'].apply(remove_punct)
#df['Rm_Punctuation']
dataframe['Words_Normalize'] = dataframe['Rm_Punctuation'].str.lower()
#df['Words_Normalize']
dataframe['Words_Tokenize'] = dataframe['Words_Normalize'].apply(word_tokenize)
#df['Words_Tokenize']
lemmatizer = WordNetLemmatizer()
dataframe['Words_Lemmatize'] = dataframe['Words_Tokenize'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])
#df['Words_Lemmatize']
dataframe.drop(columns = ['Content', 'Links', 'NONASCIIContent', 'RM_StopwordsContent', 'Rm_Punctuation', 'Words_Normalize', 'Words_Tokenize'], axis = 1, inplace = True)
dataframe.rename(columns = {'Words_Lemmatize' : '_Content'}, inplace = True)
## Returns preprocessed text corpus, document list and inverted index that returns occurrence as well as frequency of a word in each document.
class DF2Corpus(object):
    ''' DF2Corpus takes the dataframe containing text information from Wikipedia pages as the primary argument. 
        Generates inverted index and a corpus of text and titles. '''
    def __init__(self, *args, **kwargs):
        self.dataframe = dataframe
    def generateDataFrame(self, dataframe):
        ''' Generates preprocessed dataframe from the original dataframe. '''
        result = dataframe
        return result
    def generateDict(self, dataframe):
        ''' Converts the dataframe into a dictionary containing key-value pairs. Helpful in segregation of information into documents and text corpus. '''
        dictionary = {}
        for index, row in DF2Corpus.generateDataFrame(self, dataframe).iterrows():
            dictionary[row['Title']] = row['_Content']
        return dictionary
    def generateTextCorpus(self, dataframe):
        ''' Returns a list object containing a corpus of the whole plain text content from different Wikipedia pages '''
        corpusText = list(DF2Corpus.generateDict(self, dataframe).values())
        return corpusText
    def invertedIndex(self, dataframe):
        ''' Builds an inverted index. On supply of a word, returns the occurrence as well as frequency in each document. '''
        index = HashedIndex()
        for key, value in DF2Corpus.generateDict(self, dataframe).items():
             for value in DF2Corpus.generateDict(self, dataframe)[key]:
                index.add_term_occurrence(value, key)
        return index
        ## Occurrence and frequency of term can be retrieved using:
        ## df2c = DF2Corpus()
        ## df2c.invertedIndex(dataframe).get_documents(query)
    def generateTitleCorpus(self, dataframe):
        ''' Returns a list object containing the names of documents. '''
        corpusTitle = list(DF2Corpus.generateDict(self, dataframe).keys())
        return corpusTitle 
## Information Retrieval and Ranking based on query
query_input = input("Search the world's information: ")
query_inputBM25 = input('Search for the most relevant information: ').lower().split()
df2c = DF2Corpus()
corpus = df2c.generateTextCorpus(dataframe)
class queryPageRank(object):
    ''' queryPageRank searches corpus based on a search term. Uses three different models for query retrieval and ranks the most relevant result. '''
    def __init__(self, *args, **kwargs):
        self.corpus = corpus
    def corporaDictionary(self, corpus):
        ''' Maps preprocessed words in a corpus and their integer ids. '''
        corpora_dict = corpora.dictionary.Dictionary(corpus)
        return corpora_dict
    def search(self, index, query_input, top_n = 5, prints = False):
        ''' Method to search corpus. 
        Takes the index based on which to search on, query input and the number of most relevant results to be returned. '''
        bagofwordsVec = queryPageRank.corporaDictionary(self, corpus).doc2bow(query_input.lower().split())
        similarities = index[bagofwordsVec]
        similarities = [(x, i) for i, x in enumerate(similarities)]
        similarities.sort(key = lambda x: -x[0])
        results = []
        if prints:
            print(f"{query_input}\n")
        for result in similarities[:top_n]:
            if prints:
                print(f"{dataframe['Title'][result[1]]} \t {result[0]}\n")
            else:
                results.append(result[1], result[0])
        if not prints:
            return results
    def corpusBagofWords(self, corpus):
        ''' Convert document into Bag of Words format. Returns a list of tuples containing token ids and token counts. '''
        corpusBoW = [queryPageRank.corporaDictionary(self, corpus).doc2bow(text) for text in corpus]
        return corpusBoW
    def BagofWordsModel(self, corpus):
        ''' A sparse matrix reduces the space consumed in compressing the vector embeddings by retaining only non-zero entries. 
        Takes the bag of words corpus and the size of dictionary as the primary arguments. Computes cosine similarity against a corpus.
        When the index containing cosine similarity computation is supplied to search method along with the search term, relevant documents are retrieved. '''
        indexBoW = SparseMatrixSimilarity(queryPageRank.corpusBagofWords(self, corpus), num_features = len(queryPageRank.corporaDictionary(self, corpus)))
        return indexBoW
        ## Retrieve information on queries using following code:
        ## qPR = queryPageRank()
        ## qPR.search(qPR.BagofWordsModel(corpus), query_input, prints = True)
    def TfIdfModel(self, corpus):
        ''' Transforms word-document co-occurrence matrix into a locally/globally weighted TF-IDF matrix. 
        Weights to every word are accorded on the basis of frequency of word in a document and the number of documents containing the word. '''
        tfidf = TfidfModel(queryPageRank.corpusBagofWords(self, corpus))
        corpusTfIdf = tfidf[queryPageRank.corpusBagofWords(self, corpus)]
        indexTFIDF = SparseMatrixSimilarity(corpusTfIdf, num_features = len(queryPageRank.corporaDictionary(self, corpus)))
        return indexTFIDF
        ## Retrieve information on queries using following code:
        ## qPR = queryPageRank()
        ## qPR.search(qPR.TfIdfModel(corpus), query_input, prints = True)
    def LSIModel(self, corpus):
        ''' Latent Semantic Indexing Model transforms the bag of words into three decomposed matrices. 
        It also uses SparseMatrixSimilarity to compute cosine similarity against a corpus. '''
        modelLSI = LsiModel(queryPageRank.corpusBagofWords(self, corpus), id2word = queryPageRank.corporaDictionary(self, corpus).id2token)
        corpusLSI = modelLSI[queryPageRank.corpusBagofWords(self, corpus)]
        indexLSI = SparseMatrixSimilarity(corpusLSI, num_features = len(queryPageRank.corporaDictionary(self, corpus)))
        return indexLSI
        ## Retrieve information on queries using following code:
        ## qPR = queryPageRank()
        ## qPR.search(qPR.LSIModel(corpus), query_input, prints = True)
    def averageIDF(self, corpus):
        ''' Computes the average Inverse Document Frequency for usage in Okapi BM25 algorithm. '''
        f = []
        corpusSize = len(corpus)
        for sublist in corpus:
            frequencies = {}
            for word in sublist:
                if word not in frequencies:
                    frequencies[word] = 0
                else:
                    frequencies[word] += 1
                f.append(frequencies)                    
        for word, freq in frequencies.items():
            df = {}
            if word not in df:
                df[word] = 0
            else:
                df[word] += 1
        for word, freq in df.items():
            idf = {}
            idf[word] = math.log(corpusSize - freq + 0.5) - math.log(freq + 0.5)
        avgIDF = sum(map(lambda k: float(idf[k]), idf.keys())) / len(idf.keys())            
        return avgIDF

    def rankResult(self, corpus):
        ''' Uses Okapi BM25 algorithm to retrieve most relevant corpus of text relating a query term. '''
        okapiBM25 = BM25(corpus)
        scores = okapiBM25.get_scores(document = query_inputBM25, average_idf = queryPageRank.averageIDF(self, corpus))
        bestResult = corpus[scores.index(max(scores))]
        return bestResult
## On supply of query term, results can be retrieved using
## qPR = queryPageRank()    
## qPR.rankResult(corpus)        



